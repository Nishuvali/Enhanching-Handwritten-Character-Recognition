
## Layer Normalization

Layer Normalization normalizes the input across all neurons in the same layer. This technique ensures that the input values to a neural network are standardized, helping the model converge faster and improving overall stability during training. By normalizing across layers rather than batches, it is particularly effective for tasks with smaller batch sizes and sequential data.

<h3>Model Performance</h3>

![image](https://github.com/user-attachments/assets/9b7fb42b-c745-4feb-9890-03d63c6de452)
