## Batch Normalization

Batch Normalization standardizes the inputs to a layer for each mini-batch, reducing internal covariate shifts and allowing for faster training. By maintaining the mean and variance of inputs across batches, it stabilizes learning, enabling the use of higher learning rates and improving generalization in deep neural networks.

<h3>Model Performance</h3>

![image](https://github.com/user-attachments/assets/5eb8bf67-090f-46e2-92b1-bd030ecbf338)
